{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529bd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "np.random.seed(42)\n",
    "w1 = np.random.uniform(-0.5, 0.5, (2, 3))  # 2 inputs, 3 hidden neurons\n",
    "w2 = np.random.uniform(-0.5, 0.5, (3, 1))  # 3 hidden neurons, 1 output neuron\n",
    "b1, b2 = 0.5, 0.7  # Biases\n",
    "\n",
    "\n",
    "X = np.array([[0.1, 0.2]])  # Example input\n",
    "Y = np.array([[0.3]])  # Target output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a268724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output before training: [[0.6034966]]\n",
      "Output after training: [[0.3]]\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "hidden_input = np.dot(X, w1) + b1\n",
    "hidden_output = tanh(hidden_input)\n",
    "final_input = np.dot(hidden_output, w2) + b2\n",
    "final_output = tanh(final_input)\n",
    "print(\"Output before training:\", final_output)\n",
    "\n",
    "# Backpropagation\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X, w1) + b1\n",
    "    hidden_output = tanh(hidden_input)\n",
    "    final_input = np.dot(hidden_output, w2) + b2\n",
    "    final_output = tanh(final_input)\n",
    "    \n",
    "    # Compute error\n",
    "    error = Y - final_output\n",
    "    \n",
    "    # Backpropagation\n",
    "    d_final = error * tanh_derivative(final_output)\n",
    "    d_hidden = d_final.dot(w2.T) * tanh_derivative(hidden_output)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    w2 += learning_rate * hidden_output.T.dot(d_final)\n",
    "    w1 += learning_rate * X.T.dot(d_hidden)\n",
    "    b2 += learning_rate * np.sum(d_final)\n",
    "    b1 += learning_rate * np.sum(d_hidden)\n",
    "\n",
    "print(\"Output after training:\", final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0202e20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward output before training: [[0.30240427]]\n",
      "Epoch 0: Backpropagation output = [[0.30240427]]\n",
      "Epoch 200: Backpropagation output = [[0.3]]\n",
      "Epoch 400: Backpropagation output = [[0.3]]\n",
      "Epoch 600: Backpropagation output = [[0.3]]\n",
      "Epoch 800: Backpropagation output = [[0.3]]\n",
      "Forward output after training: [[0.3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2 \n",
    "\n",
    "np.random.seed(42)\n",
    "w1 = np.random.uniform(-0.5, 0.5, (2, 3))  \n",
    "\n",
    "\n",
    "X = np.array([[0.1, 0.2]])  \n",
    "Y = np.array([[0.3]])  \n",
    "\n",
    "# Forward pass (before training)\n",
    "hidden_input = np.dot(X, w1) + b1  # Compute input to hidden layer\n",
    "hidden_output = tanh(hidden_input)  # Apply activation function\n",
    "final_input = np.dot(hidden_output, w2) + b2  # Compute input to output neuron\n",
    "final_output = tanh(final_input)  # Apply activation function\n",
    "print(\"Forward output before training:\", final_output)\n",
    "\n",
    "# Backpropagation parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for _ in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X, w1) + b1  # Compute hidden layer input\n",
    "    hidden_output = tanh(hidden_input)  # Apply tanh activation\n",
    "    final_input = np.dot(hidden_output, w2) + b2  # Compute output layer input\n",
    "    final_output = tanh(final_input)  # Apply tanh activation\n",
    "    \n",
    "\n",
    "    error = Y - final_output  # Difference between actual and predicted output\n",
    "    \n",
    "    # Backpropagation\n",
    "    d_final = error * tanh_derivative(final_output)  # Gradient for output layer\n",
    "    d_hidden = d_final.dot(w2.T) * tanh_derivative(hidden_output)  # Gradient for hidden layer\n",
    "    \n",
    "    # Update weights and biases\n",
    "    w2 += learning_rate * hidden_output.T.dot(d_final)  # Update weights from hidden to output\n",
    "    w1 += learning_rate * X.T.dot(d_hidden)  # Update weights from input to hidden\n",
    "    b2 += learning_rate * np.sum(d_final)  # Update bias for output layer\n",
    "    b1 += learning_rate * np.sum(d_hidden)  # Update bias for hidden layer\n",
    "\n",
    "    # Print output during training at certain intervals\n",
    "    if _ % 200 == 0:\n",
    "        print(f\"Epoch {_}: Backpropagation output =\", final_output)\n",
    "\n",
    "# Output after training\n",
    "print(\"Forward output after training:\", final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf2e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
